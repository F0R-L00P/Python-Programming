{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premier-catalyst",
   "metadata": {},
   "source": [
    "### Using Requests and BS4 to Scrape websites\n",
    "Requests will pull down the webpage, can use BS4 to parse information off the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-exposure",
   "metadata": {},
   "source": [
    "Let's specify the target url where we will be parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protective-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://pybit.es/pages/projects.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-linux",
   "metadata": {},
   "source": [
    "### Create function to extract the webpage\n",
    "1. get page\n",
    "2. get status code if any errors are in encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to extract the webpage\n",
    "def pull_website():\n",
    "    raw_page = requests.get(url)\n",
    "    raw_page.raise_for_status()\n",
    "    #return raw data\n",
    "    return raw_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-running",
   "metadata": {},
   "source": [
    "### Create function to scrape the use agiast the site object\n",
    "1. Create soup object, take site TEXT and store in object, using bs4 html parser.\n",
    "2. The select option allows us to \"select\" something out of the html source. In this case we are interested in the project headers, can be found by inspecting page. \n",
    "3. Iterate over the list and obtain html headers. Store in header list. Once stored can use soup.select, this will get the entire tag <h3....h3>. After we must use getText to only obtain the text in between the tags.\n",
    "4. Examin the obtained text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brazilian-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(site):\n",
    "    #lets store headers in a list\n",
    "    header = []\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(site.text, 'html.parser')\n",
    "    \n",
    "    # inspected tag \n",
    "    # <h3 class='projectHeader'>0. PyBites Apps (first 100 days)</h3> \n",
    "    html_header_list = soup.select('.projectHeader')\n",
    "\n",
    "    for i in html_header_list:\n",
    "        header.append(i.getText())\n",
    "    \n",
    "    for i in header:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "approved-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. PyBites Apps (first 100 days)\n",
      "1. #100DaysOfCode (Mar 30, 2017 - Jul 07, 2017)\n",
      "2. #100DaysOfDjango (Jul 08, 2017 - Oct 15, 2017)\n",
      "3. PyBites Code Challenges Platform (Oct 16, 2017 - Jan 23, 2018)\n",
      "4. PyBites Community #100DaysOfCode (Jan 24, 2017 - May 03, 2018)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # assign page to variable site\n",
    "    site = pull_website()\n",
    "    # call the scrape function\n",
    "    scrape(site)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
